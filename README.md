# ğŸ“š RAG â€” Gemini / Ollama + Pinecone/Chroma (Streamlit)

Dual-mode Retrieval-Augmented Generation (RAG) app:

- **Gemini mode** â†’ Google AI `text-embedding-004` + **Pinecone** vector DB + (Gemini 2.5 Flash) generation
- **Ollama mode** â†’ Local embeddings (e.g., `nomic-embed-text`) + **ChromaDB** vector DB + (Mistral) generation

## âœ¨ Features
- Upload **PDF/Markdown** in the UI or index everything in a **docs/** folder
- **Citations** for each answer
- **Streaming** responses (Ollama path; Gemini falls back if SDK streaming is unavailable)
- **Guardrails**: profanity filter + similarity threshold hallucination gate
- **Delta ingest**: only new/changed files re-embedded
- **Namespaces**: keep multiple datasets separate
- **Latency** display (ms)

> **Supported files**: `.pdf`, `.md`, `.markdown`

---

## ğŸ§± Project Structure

```
.
â”œâ”€ app/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ ingest.py          # loads/cleans/splits docs into chunks
â”‚  â”œâ”€ rag.py             # dual-mode RAG core (Gemini/Pinecone or Ollama/Chroma)
â”‚  â”œâ”€ state.py           # per-namespace tracking for delta ingest
â”‚  â”œâ”€ utils.py           # settings loader, timers, helpers
â”‚  â”œâ”€ filters.py         # profanity + hallucination gate
â”œâ”€ docs/                 # optional: sample docs; also used by UI uploads
â”œâ”€ chroma_store/         # local Chroma DB (ignored by git)
â”œâ”€ state/                # ingest tracking (ignored by git)
â”œâ”€ .streamlit/
â”‚  â””â”€ config.toml
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â”œâ”€ streamlit_app.py      # main UI
â”œâ”€ .env.example          # template (no secrets)
â””â”€ README.md
```

---

## ğŸ”‘ Environment

Copy `.env.example` â†’ `.env` (donâ€™t commit `.env`). Fill:

```env
# Default provider used on app start (can switch in UI)
LLM_PROVIDER=gemini    # gemini | ollama

# Google AI (Gemini)
GEMINI_API_KEY=PUT_YOUR_KEY_HERE

# Pinecone (for Gemini mode)
PINECONE_API_KEY=PUT_YOUR_KEY_HERE
PINECONE_INDEX=rag-index
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Chunking (better recall with smaller overlap)
CHUNK_SIZE=600
CHUNK_OVERLAP=150

# Retrieval defaults
TOP_K=5
MIN_SIM_THRESHOLD=0.55
PROFANITY_BLOCK=1

# Ollama (local mode)
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=mistral
OLLAMA_EMBED_MODEL=nomic-embed-text

# Chroma local store path
CHROMA_DIR=./chroma_store
```

---

## ğŸ› ï¸ Install & Run (Local)

```bash
# 1) Create env and install deps
python -m venv .venv
# or: conda create -n rag_llm python=3.10
pip install -r requirements.txt

# 2) (Optional) for local mode
#    install & run Ollama, then pull models:
#    https://ollama.com/download
ollama serve
ollama pull mistral
ollama pull nomic-embed-text

# 3) Run the app
streamlit run streamlit_app.py
```

---

## ğŸ–¥ï¸ Using the App

1. Choose **LLM provider** in sidebar:
   - **Gemini** â†’ embeddings + Pinecone (cloud)
   - **Ollama** â†’ local embeddings + Chroma (local)
2. Upload files (PDF/MD) or place them in the **Documents folder** (default `docs/`)
3. Click **Save uploads to folder** (for uploaded files)
4. Click **Scan & Ingest (delta)** to index the folder
5. Ask questions â†’ answers cite sources. Adjust **Top-K** and **Min similarity** as needed.

**Sidebar tips**
- **Top-K**: 3â€“7 (start with 5)
- **Min similarity**: 0.50â€“0.60 for small corpora
- Namespaces keep datasets separate (e.g., `productA`, `productB`).

---

## â˜ï¸ Deploy on Streamlit Community Cloud

> **Note:** Ollama (local) doesnâ€™t run on Streamlit Cloud. Deploy **Gemini + Pinecone** mode.

1. Push repo to GitHub.
2. Streamlit Cloud â†’ **New app** â†’ select your repo/branch â†’ `streamlit_app.py`.
3. Add **Secrets** (App â†’ Settings â†’ Secrets), e.g.:
   ```toml
   GEMINI_API_KEY = "YOUR_KEY"
   PINECONE_API_KEY = "YOUR_KEY"
   PINECONE_INDEX = "rag-index"
   PINECONE_CLOUD = "aws"
   PINECONE_REGION = "us-east-1"
   LLM_PROVIDER = "gemini"
   TOP_K = "5"
   MIN_SIM_THRESHOLD = "0.55"
   CHUNK_SIZE = "600"
   CHUNK_OVERLAP = "150"
   ```
4. Deploy. Use the **Upload** widget or commit docs to the repo.

---

## ğŸ” Troubleshooting

- **Gemini embeddings fail** â†’ ensure `google-genai>=0.2.0` and `GEMINI_API_KEY` is set.
- **Pinecone empty** â†’ create index via the top status panel (768, cosine) and check region/cloud.
- **Ollama embeddings mismatch** â†’ run `ollama serve` and `ollama pull nomic-embed-text`.
- **Negative scores in Ollama mode** â†’ collection must be cosine; we force `metadata={'hnsw:space':'cosine'}` and normalize vectors. Drop namespace and re-ingest.
- **Low recall** â†’ set `CHUNK_SIZE=600`, `CHUNK_OVERLAP=150`, `TOP_K=5`, `MIN_SIM_THRESHOLD=0.55`.

---

## ğŸ” Security / Secrets

- Donâ€™t commit real API keys. Use `.env` locally and **Secrets** on Streamlit Cloud.
- `.gitignore` excludes `.env`, `state/`, and `chroma_store/`.

---

## ğŸ“œ License

MIT Â© 2025 Your Name
