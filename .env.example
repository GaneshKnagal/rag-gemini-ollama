# Provider selection (default at runtime; override in UI)
LLM_PROVIDER=gemini  # gemini | ollama

# Google AI (Gemini)
GEMINI_API_KEY=AIzaSyBmipetvlV9fKm8jnx8vQ1putlaeKK2JAQ

# Pinecone (Gemini mode)
PINECONE_API_KEY=pcsk_44zEye_ShANiC7d8csnC6eYnrN6LqwwQUia4DF3a6frePNJ7fKcD5iJVpRxvxncTurdjgJ
PINECONE_INDEX=rag-index
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Chunks (smaller + overlap for better recall)
CHUNK_SIZE=600
CHUNK_OVERLAP=150

# Retrieval defaults
TOP_K=5
MIN_SIM_THRESHOLD=0.55
PROFANITY_BLOCK=1

# Ollama (local mode)
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=mistral
OLLAMA_EMBED_MODEL=nomic-embed-text

# Chroma local store path
CHROMA_DIR=./chroma_store

